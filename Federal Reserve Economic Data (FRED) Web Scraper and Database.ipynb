{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federal Reserve Economic Data (FRED) Web Scraper and Database \n",
    "\n",
    "***\n",
    "**Purpose:** The goal of the project is to gather historical economic data from the Federal Reserve Economic Data (FRED) website and store it in a database. The data will be used to augment other datasets for future modeling efforts. <br>\n",
    "\n",
    "**Gathering the data:** The data on the FRED is not represented in tabular format. The data for each indicator is displayed as a graph on its own page. The below code makes use of the FRED API to pull the historic values for each indicator. However, the FRED requires that each indicator is queried individually using a unique indicator ID. Unfortunately, a complete listing of the indicator IDs is not available in the API documentation. The BeautifulSoup package was used to create a web crawler to parse the FRED website and obtain a complete listing of the unique IDs for the economic indicators available. With the complete listing of unique IDs, we were able to use the API in conjunction with pandas to construct a data frame where each economic indicator is represented as a column and each row is a date. <br>\n",
    "\n",
    "**Storing the data & automation of refreshing the database with updates to indicators:** It will be helpful for future modeling efforts to be able to access the economic data quickly. However, gathering all the indicators for a long reporting period can be time consuming. To avoid lengthy queries in the future, all the economic data was stored to a database. Additionally, a script is available to quickly refresh the database with newly reported metrics as they become available. <br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from fred import Fred\n",
    "from datetime import date,timedelta\n",
    "import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "** Obtaining API Key and Accessing the FRED ** https://research.stlouisfed.org/useraccount/login/secure/\n",
    "<br>\n",
    "## A. Scraping the Data From the FRED and Creating a Database\n",
    "### A1. Use BeautifulSoup to scrape the FRED website\n",
    "The below codes parses the FRED website to gather unique economic indicator IDs and description in key, value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BeautifulSoup to get Series IDs\n",
    "site = 'https://fred.stlouisfed.org/tags/series?ob=pv'\n",
    "html = urlopen(site)\n",
    "soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "# Note: the 'a' tags containing the series_ids and titles\n",
    "# Lists to store names and ids\n",
    "series_ids = []\n",
    "series_names = []\n",
    "\n",
    "# Populate the list via parsing 'a' tags\n",
    "while True:\n",
    "    link_objs = soup.findAll('a',{'class':'series-title'})\n",
    "\n",
    "    for link in link_objs:\n",
    "        ids = str(link.attrs['href']).replace('/series/','')\n",
    "        names = link.get_text()\n",
    "        series_ids.append(ids)\n",
    "        series_names.append(names)\n",
    "    try:\n",
    "        site = soup.find('a',{'title':'next page'}).attrs['href']\n",
    "        html = urlopen(site)\n",
    "        soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "    except:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique IDs and descriptions will be exported as pickle to avoid having to run the code again when refreshing the database in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('D:/Analytics/Fall Semester/Machine Learning/Machine Learnings Class 1/series_ids', 'wb')\n",
    "pickle.dump(series_ids, file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "file.close()\n",
    "del series_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Pull the Economic Indicator Values Using the FRED API\n",
    "The following code uses the series ids scraped in the above code to query the indicators using the FRED API.\n",
    "The code iteratively pulls the values for each indicator adds it as an individual column to a dataframe. <br><br>\n",
    "*Note:* The indicators from 1/1/1975 to 12/31/2016 will be queried now. Data from 2017 and 2018 will be extracted with the script to update the database in future steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date = datetime.date(1975,1,1)\n",
    "to_date = datetime.date(2016,12,31)\n",
    "str_from_date = str(from_date)\n",
    "str_to_date = str(to_date)\n",
    "\n",
    "# Variables\n",
    "series_ids = pickle.load(open('file_location', 'rb'))\n",
    "series_list =[]\n",
    "\n",
    "# Get all of the series for the time period requested\n",
    "fr = Fred(api_key='###',response_type='dict')\n",
    "\n",
    "\n",
    "#Indictor ID and Name Change\n",
    "for series in series_ids:\n",
    "    series_list.append(fr.series.observations(series_id = series,\n",
    "                                              observation_start = str_from_date,\n",
    "                                              observation_end= str_to_date))\n",
    "# Generate the datelist\n",
    "date_list=[]\n",
    "delta = to_date - from_date\n",
    "for i in range(delta.days+1):\n",
    "    x = from_date + timedelta(days = i)\n",
    "    date_list.append(str(x))\n",
    "\n",
    "# Create the dataframe\n",
    "indicator_df = pd.DataFrame({'DATE':date_list})\n",
    "\n",
    "for index, series in enumerate(series_list):\n",
    "    try:\n",
    "        #extract series infomration\n",
    "        df = pd.DataFrame(series)\n",
    "        df = df[['date', 'value']]\n",
    "        df.date = df.date.astype(str)\n",
    "        df = df.rename(columns = {'date':'DATE','value':series_ids[index]})\n",
    "        indicator_df = indicator_df.merge(df, how='left', on= 'DATE')\n",
    "    except:\n",
    "        fake_df = pd.DataFrame({'DATE':date_list})\n",
    "        fake_df['value'] = np.nan\n",
    "        fake_df = fake_df.rename(columns={'date': 'DATE', 'value': series_ids[index]})\n",
    "        indicator_df = indicator_df.merge(fake_df , how='left', on='DATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Clean the Data Frame\n",
    "The below will remove any indicators that are completely null. The indicators are all null because the time period queried did correspond to the time period these indicators were reported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for columns with all nans\n",
    "all_null = pd.isnull(indicator_df).sum() == indicator_df.shape[0]\n",
    "\n",
    "# Filter the columns with all missing values\n",
    "all_null = all_null[all_null ==True]\n",
    "all_null_list = list(all_null.index)\n",
    "\n",
    "# Remove these columns from the dataframe\n",
    "new_indicator_df = indicator_df.drop(all_null_list, axis = 1)\n",
    "\n",
    "# Remove the all_nulls IDs from the listing\n",
    "new_series_ids = [i for i in series_ids if i not in all_null_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4. Push the Data Frame to a SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to Heidi SQL database\n",
    "# Connection engine ('mysql + pymysql://username:password@localhost:port/database_name)\n",
    "engine =create_engine('mysql+pymysql://python:py123@localhost:3306/econ') #connects to server\n",
    "\n",
    "# Creates the connection\n",
    "connection = engine.connect()\n",
    "new_indicator_df.to_sql('indicators', con = engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Update the Database with Newly Reported Economic Data<br>\n",
    "### B1.  Automate Refreshing the Database with New Information\n",
    "The below code updates the database with newly reported economic data that has not been previously captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"UPDATE LOOP\"\"\"\n",
    "#get dates\n",
    "from_date = datetime.date(2016,12,1)\n",
    "to_date = datetime.date(2017,6,30)\n",
    "str_from_date = str(from_date)\n",
    "str_to_date = str(to_date)\n",
    "\n",
    "\n",
    "#variables\n",
    "series_list =[]\n",
    "\n",
    "# Get all of the series for the time period requested\n",
    "fr = Fred(api_key='###',response_type='dict')\n",
    "\n",
    "\n",
    "# Indictor ID and Name Change\n",
    "for series in new_series_ids:\n",
    "    series_list.append(fr.series.observations(series_id = series,\n",
    "                                              observation_start = str_from_date,\n",
    "                                              observation_end= str_to_date))\n",
    "# Generate the datelist\n",
    "date_list=[]\n",
    "delta = to_date - from_date\n",
    "for i in range(delta.days+1):\n",
    "    x = from_date + timedelta(days = i)\n",
    "    date_list.append(str(x))\n",
    "\n",
    "# Create the Dataframe\n",
    "update_df = pd.DataFrame({'DATE':date_list})\n",
    "\n",
    "for index, series in enumerate(series_list):\n",
    "    try:\n",
    "        #extract series infomration\n",
    "        df = pd.DataFrame(series)\n",
    "        df = df[['date', 'value']]\n",
    "        df.date = df.date.astype(str)\n",
    "        df = df.rename(columns = {'date':'DATE','value':new_series_ids[index]})\n",
    "        update_df = update_df.merge(df, how='left', on= 'DATE')\n",
    "    except:\n",
    "        fake_df = pd.DataFrame({'DATE':date_list})\n",
    "        fake_df['value'] = np.nan\n",
    "        fake_df = fake_df.rename(columns={'date': 'DATE', 'value': new_series_ids[index]})\n",
    "        update_df = update_df.merge(fake_df , how='left', on='DATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Data Cleaning and User Error Handling\n",
    "The following code reviews the table for duplicate dates and drops the duplicate record. This is a safeguard against user error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dates to dt.date\n",
    "update_df.DATE = pd.to_datetime(update_df.DATE)\n",
    "update_df.DATE = update_df.DATE.dt.date\n",
    "\n",
    "# Get unique dates from dataframe\n",
    "df_dates = update_df.DATE.tolist()\n",
    "\n",
    "# Get unique dates from sql database\n",
    "sql = \"\"\"SELECT DISTINCT date FROM indicators\"\"\"\n",
    "proxy_result = connection.execute(sql)\n",
    "results = proxy_result.fetchall()\n",
    "\n",
    "# Converts the results from query to a list \n",
    "table_dates = [i[0] for i in results]\n",
    "\n",
    "# Get only the dates not included in the dataframe\n",
    "load_dates = [i for i in df_dates if i not in table_dates]\n",
    "\n",
    "# Retrieve unique update\n",
    "unique_update = update_df[update_df.DATE.isin(load_dates)]\n",
    "\n",
    "# Pull the whole table from the database\n",
    "whole_table = pd.read_sql('SELECT * FROM indicators', con =  engine)\n",
    "whole_table.DATE =pd.to_datetime(whole_table.DATE)\n",
    "whole_table.DATE = whole_table.DATE.dt.date\n",
    "\n",
    "# Outer join unique_update to whole table\n",
    "whole_table = whole_table.merge(unique_update,how='outer')\n",
    "\n",
    "# Drop table before pushing to SQL Database\n",
    "connection.execute('DROP TABLE indicators;')\n",
    "\n",
    "# Update the Table\n",
    "whole_table.to_sql('indicators', con = engine, if_exists = 'replace', index = False)\n",
    "del unique_update\n",
    "del whole_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3 Query Code\n",
    "The below script can be used to query the economic data frame the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the query\n",
    "query_table = pd.read_sql(\"\"\"SELECT * FROM indicators\n",
    "WHERE DATE BETWEEN '2017/01/01' AND '2017/06/20';\"\"\", con= engine)\n",
    "\n",
    "# Conver date to datetime\n",
    "query_table.DATE =pd.to_datetime(query_table.DATE)\n",
    "query_table.DATE = query_table.DATE.dt.date\n",
    "\n",
    "# Rollfoward previous data when the indictor is not reported on a specific day\n",
    "query_table = query_table.fillna(method='ffill')\n",
    "\n",
    "# Extract only quarterly data\n",
    "quarter_range = pd.date_range(start = '2016/01/01', end = '2017/10/01' ,freq = 'qs')\n",
    "quarter_range = quarter_range.date.tolist()\n",
    "return_table = query_table[query_table.DATE.isin(quarter_range)]\n",
    "return_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "563px",
    "left": "1572.43px",
    "right": "20px",
    "top": "121px",
    "width": "466px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
